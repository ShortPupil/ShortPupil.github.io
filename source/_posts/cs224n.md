---
title: cs224n笔记
tags:
  - nlp
copyright: true
categories: 自然语言处理
abbrlink: 25b610a6
date: 2021-06-30 14:23:24
---



## 二、词的向量表示

### 计算机如何处理词语的意思

#### Distributional similarity based representations

语言学家J. R. Firth提出，通过一个单词的上下文可以得到它的意思。J. R. Firth甚至建议，如果你能把单词放到正确的上下文中去，才说明你掌握了它的意义。

这是现代统计自然语言处理最成功的思想之一

#### 通过向量定义词语的含义

通过调整一个单词及其上下文单词的向量，使得根据两个向量可以推测两个词语的相似度；或根据向量可以预测词语的上下文。

**递归**，根据向量来调整向量，与词典中意项的定义相似。

#### 学习神经网络word embedding的基本思路

定义一个以预测某个单词的上下文的模型：

$$
p(context |w_t) = x
$$
损失函数定义如下：

$$
J=1−p(w_{−t}|w_t)
$$

这里的$w_{−t}$表示$w_t$的上下文（负号通常表示除了某某之外），如果完美预测，损失函数为零。

然后在一个大型语料库中的不同位置得到训练实例，调整词向量，最小化损失函数。

#### 直接学习低维词向量

论文如下

• Learning representations by back-propagating errors (Rumelhart et al., 1986)

• **A neural probabilistic language model (Bengio et al., 2003)**

• NLP (almost) from Scratch (Collobert & Weston, 2008)

• A recent, even simpler and faster model: word2vec (Mikolov et al. 2013) 



### word2vec的主要思路

word2vec——神经概率语言模型的输入，是为了通过神经网络学习**某个分类模型（“CBOW”和“Skip-gram”）**而产生的中间结果，而这个具体学习过程会用到**两个降低复杂度的近似方法（Hierarchical Softmax或Negative Sampling）**

两个语言模型

![屏幕快照 2016-07-14 下午7.27.35.png](https://ww3.sinaimg.cn/large/6cbb8645gw1f5to6e5d9lj216c0qkwhk.jpg)



#### Hierarchical softmax

##### Continuous Bag of Words (CBOW)

预测目标单词，根据上下文的词语预测当前词语的出现概率的模型。已知上下文，估算当前词语的语言模型。

其学习目标是**最大化对数似然函数**

**输入层 INPUT**：上下文的词语的词向量（不训练词向量；而是训练CBOW模型，词向量是CBOW模型的一个参数，训练开始时，词向量是一个输入的随机值，随着训练的进行不断被更新）

**投影层 PROJECTION**：求和，简单的向量加法

**输出层 OUTPUT**：输出最可能的w——这个过程算是**多分类**问题，给定特征，从|C|个分类中挑一个？

对于神经网络模型多分类，最朴素的做法是**softmax回归**

![神经网络依存句法分析29.png](https://ww2.sinaimg.cn/large/6cbb8645gw1exxdsuugv1j20cl033jrj.jpg)

另有方法 **SVM的多分类**的二叉树结构应用到word2vec 即是**Hierarchical Softmax算法**



树状结构（用于CBOW的输出层）

![svm_多分类.gif](https://ww1.sinaimg.cn/large/6cbb8645gw1f5wmvf9tbrg20bf08mq30.gif)

![屏幕快照 2016-07-17 上午9.13.40.png](https://ww3.sinaimg.cn/large/6cbb8645gw1f5wmy4jdnwj214w12a42v.jpg)

复杂的数学计算......

![屏幕快照 2016-07-17 上午11.15.50.png](https://ww1.sinaimg.cn/large/6cbb8645gw1f5wqgz0elqj20pm0qaq5a.jpg)

##### Skip-grams (SG)

预测上下文

Skip-gram只是逆转了CBOW的因果关系而已，即已知当前词语，预测上下文。

![屏幕快照 2016-07-17 上午11.33.31.png](https://ww1.sinaimg.cn/large/6cbb8645gw1f5wqzg68u0j214a120wij.jpg)

- 输入层不再是多个词向量，而是一个词向量
- 投影层其实什么事情都没干，直接将输入层的词向量传递给输出层

##### 总结

无论是CBOW还是Skip-gram模型，其实都是分类模型。对于机器学习中的分类任务，在训练的时候不但要给正例，还要给负例。对于Hierarchical Softmax，负例是二叉树的其他路径。对于Negative Sampling，负例是随机挑选出来的。



#### Negative sampling

##### CBOW

![屏幕快照 2016-07-17 下午1.55.05.png](https://ww4.sinaimg.cn/large/6cbb8645gw1f5wv2jzxnfj20oe0ncdhp.jpg)

##### Skip-gram

![屏幕快照 2016-07-17 下午2.40.41.png](https://ww4.sinaimg.cn/large/6cbb8645gw1f5wwegdnryj20oi0q00v3.jpg)



## 三、高级词向量表示

#### 总结

- 遍历整个语料库的每个词
- 预测每个词的上下文
- ![hankcs.com 2017-06-08 下午3.25.31.png](https://wx1.sinaimg.cn/large/006Fmjmcly1fgdtp2e4e2j30uy0bmn6v.jpg)
- 每个窗口中计算梯度做SGD



每个窗口最多2m+1个单词，所以$$∇_θJ_t(θ)$$会非常稀疏

![hankcs.com 2017-06-08 下午3.33.16.png](https://wx2.sinaimg.cn/large/006Fmjmcly1fgdtx6wu43j30ta0qegnj.jpg)

实际上有正确答案需要去对比的，只有**窗口中的词语**。

所以每次更新只更新WW矩阵中的少数列，或者为每个词语建立到词向量的哈希映射：![hankcs.com 2017-06-08 下午3.39.45.png](https://wx3.sinaimg.cn/large/006Fmjmcly1fgdu3xf2phj30qo09a755.jpg)



另一个难点在于，词表V的量级非常大，以至于分母很难计算![hankcs.com 2017-06-08 下午3.25.31.png](https://wx1.sinaimg.cn/large/006Fmjmcly1fgdtp2e4e2j30uy0bmn6v.jpg)

因此assignment 1用negative sampling实现skip-gram，这是一种采样子集简化运算的方法。

具体做法是，**对每个正例（中央词语及上下文中的一个词语）采样几个负例（中央词语和其他随机词语），训练binary logistic regression（也就是二分类器）**。

 目标函数：$$J_t(\theta) = \log \sigma\left(u_o^Tv_c\right)+\sum_{i=1}^k\mathbb{E}_{j \sim P(w)}\left [\log\sigma \left(-u^T_jv_c\right) \right]$$

这里t是某个窗口，k是采样个数，P(w)是一个unigram分布，σ是sigmoid函数

![sigmoid.png](https://wx4.sinaimg.cn/large/006Fmjmcly1fgdvx2wor0j30f90ai0t5.jpg)

 



